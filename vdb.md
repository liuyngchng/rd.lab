## 1. 目标

这篇文档的目的，是以最简单的例子，说明文本向量化的过程。

## 2. 基本概念

（1）向量。一组有序的数值， 例如向量 a = [0.1, -0.3, 0.4]

（2）向量的纬度。一组有序数值的个数， 例如向量 a = [0.1, -0.3, 0.4]， a 的纬度为3纬。

（3）矩阵 (Matrix)。多个纬度相同的向量的组合体。

```
[
	[0.1, -0.3, 0.4]
	[0.2, -0.12, 0.8]
]
```

## 3. 文本向量化过程

整个文本的向量化过程如下

```text
原始文本 → 	预处理 	→  分词 	→ 模型编码 → 池化 	→ 归一化 → 输出向量
    ↓        ↓          ↓          ↓        ↓        ↓        ↓
1000字     清洗文本    Token序列  隐藏状态     聚合    标准化   768维向量
```

### 3.1 预处理

（1）文本清洗

```
#    - 去除多余空白字符
#    - 标准化标点（中文全角转半角等）
#    - 特殊字符处理
```

（2）长度控制

```
# BGE-M3最大支持8192 tokens，你的1000字约需要：
#   中文字符：1000个汉字 ≈ 1000-1500 tokens
#   英文单词：按平均5字母算，1000字≈200单词 ≈ 200-300 tokens
# 所以完全在模型处理范围内
```

/// todo : 如果长度超过了 8192个 tokens 怎么办？

### 3.2 分词（Tokenization）

#### 3.2.1 分词过程

```
# 假设文本："自然语言处理是人工智能的重要分支"

# 分词结果可能是（展示部分）：
tokens = [
    '▁自然',     # "▁"表示空格或词开头
    '语言',
    '处理',
    '是',
    '人工',
    '智能',
    '的',
    '重要',
    '分支'
]

# 实际会得到约 600-800 个 token（1000字）
# 每个 token 被映射为一个数字 ID：
token_ids = [1245, 3567, 2890, 15, 4567, 2341, 8, 3456, 2789, ...]

# 添加特殊标记：
# [CLS] - 分类标记（开头）   token_id: 0
# [SEP] - 分隔标记（结尾）   token_id: 2

# 最终输入模型的序列：
input_ids = [0, 1245, 3567, 2890, 15, 4567, 2341, 8, 3456, 2789, ..., 2]
# 长度约为 600-800
```

#### 3.2.2 词汇表

一大段文本首先分成一个词语的数据， 然后将每个词语转换为一个数字，这个过程是通过 NLP中的**词汇表映射（Vocabulary Mapping）**进行的。 以 BGE-M3的词汇表（XLM-RoBERTa base，约250,000个词条）为例， 如词汇表2-1所示。

```

vocabulary = {
    0: "<s>",        # 句子开始（CLS）
    1: "<pad>",      # 填充符
    2: "</s>",       # 句子结束（SEP）
    3: "<unk>",      # 未知词
    4: "▁",          # 空格前缀
    5: "。",
    6: "，",
    7: "的",
    8: "是",
    9: "一",
    10: "在",
    11: "有",
    12: "了",
    13: "人",
    ...
    1245: "▁自然",   # 注意：这个编号是假设的，假设 "▁自然" 对应的数字为1245
    ...
    2890: "处理",
    ...
    3567: "语言",
    ...
    4567: "人工",
    2341: "智能",
    3456: "重要",
    2789: "分支",
    ...
    250000: "▁transformer"  # 最后一个词
}
```

<div align='center'><b>词汇表 2-1 vocabulary 举例</b></div>

token 到 id 的映射过程如下

```python
# 你的原始文本
text = "自然语言处理是人工智能的重要分支"

# 1. 分词（Tokenization）
tokens = ['▁自然', '语言', '处理', '是', '人工', '智能', '的', '重要', '分支']

# 2. 查表映射（Vocabulary Lookup）
# 模型内部做的其实是：
token_ids = []
for token in tokens:
    # 去词汇表里找这个token对应的ID
    token_id = vocabulary.get(token, 3)  # 3是<unk>的ID
    token_ids.append(token_id)

# 得到：
# '▁自然' → 1245
# '语言'  → 3567
# '处理'  → 2890
# '是'    → 8      # 常见词ID小
# '人工'  → 4567
# '智能'  → 2341
# '的'    → 7      # 非常常见，ID很小
# '重要'  → 3456
# '分支'  → 2789

token_ids = [1245, 3567, 2890, 8, 4567, 2341, 7, 3456, 2789]
```

#### 3.2.3 词汇表来源

（1）首先进行数据收集，例如用海量文本（维基百科、书籍、网页等）统计

（2）优化词表。 使用SentencePiece的Unigram算法， 找到最优的词表，使得所有训练文本的"似然概率"最大。

（3）词语切分。

```python
text = "自然语言处理"
# 可能的切分：
#   ["自", "然", "语", "言", "处", "理"]  # 太细碎
#   ["自然", "语言", "处理"]              # 刚刚好
#   ["自然语言", "处理"]                  # 可能更好
# 算法会选择信息量最大、覆盖最好的词表
```

（4）输出词汇表。 

```python
# BGE-M3词汇表示例（实际前20个token）：
{
    0: "<s>",        # special token: 句子开始
    1: "<pad>",      # special token: 填充
    2: "</s>",       # special token: 句子结束  
    3: "<unk>",      # special token: 未知词
    4: "▁",          # 空格前缀（重要！）
    5: "。",
    6: "，",
    7: "的",         # 中文最高频词
    8: "是",
    9: "一",
    10: "在",
    11: "有",
    12: "了",
    13: "人",
    14: "不",
    15: "中",
    16: "大",
    17: "为",
    18: "上",
    19: "个",
    20: "和",
    ...
    250: "the",      # 英文词也开始出现
    251: ".",
    252: ",",
    253: "a",
    ...
    1000: "▁中国",   # 常见实体
    1001: "▁美国",
    ...
    5000: "▁深度",   # 技术词汇
    5001: "▁学习",
    5002: "▁神经网络",
    ...
    20000: "▁transformer",  # 较新术语
    20001: "▁gpt",
    ...
    250000: "▁z"     # 最后一个
}
```

（5）这些数字ID的含义。一般来说， 这些数字符合以下规律。

```python
# 规律1：特殊标记在最前面
#   ID 0-3: 特殊标记（<s>, <pad>, </s>, <unk>）

# 规律2：单字节字符（包括中文标点）
#   ID 4: "▁"（空格前缀）
#   ID 5-100: 常见标点、单字

# 规律3：按频率排序（大致）
#   高频词在前：的(7)、是(8)、一(9)...
#   低频词在后：生僻字、专业术语

# 规律4：跨语言统一
#   中文、英文、日文等都在同一个词汇表里
```

#### 3.2.4 ID变向量

```python
# 模型有一个"嵌入矩阵"（Embedding Matrix）
# 形状：[vocab_size=250000, hidden_dim=768]

embedding_matrix = [
    [0.12, -0.05, 0.23, ...],  # ID 0 (<s>)的768维向量
    [0.08, 0.15, -0.11, ...],  # ID 1 (<pad>)的向量
    [-0.03, 0.21, 0.14, ...],  # ID 2 (</s>)的向量
    ...
    [0.34, -0.22, 0.18, ...],  # ID 1245 ("▁自然")的向量 ← 这是关键！
    ...
    [0.27, 0.31, -0.09, ...],  # ID 3567 ("语言")的向量
    ...
]  # 总计250000行，每行768个数字

# 当模型看到 token_id = 1245 时：
word_vector = embedding_matrix[1245]  
# 得到： [0.34, -0.22, 0.18, 0.05, ...] (768维)

# 这个过程类似于：
# 词语 → 身份证号(ID) → 根据身份证号去档案库找完整档案(向量)
```

过程如下所示

```python
词汇表（字典）         嵌入矩阵（向量仓库）
┌──────────────┐     ┌─────────────────┐
│ "▁自然":1245  │────▶│ 行1245: [0.34, -0.22, ... ]  │
│ "语言" :3567  │────▶│ 行3567: [0.27, 0.31, ...  ]  │
│ "处理" :2890  │────▶│ 行2890: [-0.15, 0.08, ... ]  │
│ "是"	:8     │────▶│ 行8:    [0.12, 0.05, ...  ]  │
└──────────────┘     └─────────────────┘
         ↓                     ↓
    文本中的词         	模型内部的数学表示
```

### 3.3 池化

一段文本经过分词以后，变成了一个 768纬度的向量列表,此时需要进行池化。经过2.2节的处理之后，会形成如下的结果。

```
# 假定有2段文字, txt1, txt2
txt1: 800字 → 100个token → 形状: 100 × 768  
txt2: 300字 → 150个token → 形状: 150 × 768
```

目前得到了"token级别的向量"，接下来通过池化操作形成最终的"文本向量"，池化过程如下所示

```
# txt1的token矩阵（100×768）,100行，768列
┌─────────────────────────────┐
│ token1: [0.1, 0.2, ..., 0.3] │ ← 第1行
│ token2: [0.2,-0.1, ..., 0.4] │ ← 第2行
│ token3: [-0.1,0.3,...,-0.2]  │ ← 第3行
│ ... 97 more rows ...        │
│ token100:[0.0,0.1,...,0.2]  │ ← 第100行
└─────────────────────────────┘

# 开始池化（以平均池化为例）

# txt1文本向量（1×768），即形成1行， 768列的矩阵，1个向量
 [0.05, 		0.15, 	..., 	0.25]  
   ↑     		  ↑        		 ↑
 对100行的  	 对100行的  		对100行的
 第1列取平均    第2列取平均        第768列取平均
```

也就是说，最终不管是800字，还是300字，都是转换为一个1行，768列的矩阵，即都是1个向量。

### 3.4 归一化

归一化是为了方便进行向量的计算。向量有大小（**模长**），有方向，通过归一化之后，相当于将所有的向量长度变成一样的。后面的计算只关心方向。举例如下所示。

```python
# 假设有两个向量（简化到2维），这两个向量实时上方向相同
vector_a = [3.0, 4.0]    # 模长 = √(3²+4²) = 5.0
vector_b = [1.5, 2.0]    # 模长 = √(1.5²+2²) ≈ 2.5

# 不归一化直接计算点积（内积）：
dot_product = np.dot(vector_a, vector_b)  # 3*1.5 + 4*2.0 = 4.5+8=12.5

# 问题：这个12.5既包含方向相似，也包含长度差异
# 但我们只关心方向（语义相似），不关心长度
```

进行归一化之后

```python
# L2归一化（使模长=1）
norm_a = vector_a / np.linalg.norm(vector_a)  # [3/5, 4/5] = [0.6, 0.8]
norm_b = vector_b / np.linalg.norm(vector_b)  # [1.5/2.5, 2/2.5] = [0.6, 0.8]

# 现在计算点积，点积为1 ，表示这两个向量方向完全相同
similarity = np.dot(norm_a, norm_b)  # 0.6*0.6 + 0.8*0.8 = 0.36+0.64=1.0
```

这就是归一化的作用



### 3.5 输出文本的向量